<!DOCTYPE HTML>
<html lang="en">
  <head>
    <!-- Google tag (gtag.js) -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-YKQQNE6PX6"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());

      gtag('config', 'G-YKQQNE6PX6');
    </script>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

    <title>Joochan Kim</title>

    <meta name="author" content="Joochan Kim">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <link rel="shortcut icon" href="images/favicon/favicon.ico" type="image/x-icon">
    <link rel="stylesheet" type="text/css" href="stylesheet.css">
    
  </head>

  <body>
    <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
      <tr style="padding:0px">
        <td style="padding:0px">
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr style="padding:0px">
              <td style="padding:2.5%;width:63%;vertical-align:middle">
                <p class="name" style="text-align: center;">
                  Joochan Kim
                </p>
                <p>I'm a research intern at <a href="https://cai.kist.re.kr/">KIST</a> in Seoul, mentored by <a href='https://www.hwasup.net/'>Hwasup Lim</a> and Taekgeun You, where I am working on Embodied AI, mostly focusing on Vision-Language-Action models.
                </p>
                <p>
                  I finished my master's degree at <a href="http://cse.snu.ac.kr">SNU</a>, where I was advised by <a href="https://cse.snu.ac.kr/people/faculty/55">Byoung-Tak Zhang</a>.
                  During my master course, I did internship at <a href="https://www.a-star.edu.sg/simtech/research/adaptive-robotics-and-mechatronics-(arm)">A*STAR</a> mentored by <a href='https://h-y-zhu.github.io/'>Haiyue Zhu</a>.
                  Also, I did my bachelor's degree at <a href="https://cs.yonsei.ac.kr/csai/index.do">Yonsei University</a>.
                </p>
                <p style="text-align:center">
                  <a href="mailto:tikatoka@snu.ac.kr">Email</a> &nbsp;/&nbsp;
                  <a href="data/jckim-cv.pdf">CV</a> &nbsp;/&nbsp;
                  <a href="https://scholar.google.com/citations?user=8DtoB2QAAAAJ">Scholar</a> &nbsp;/&nbsp;
                  <a href="https://www.linkedin.com/in/tikatoka/">LinkedIn</a> &nbsp;/&nbsp;
                  <a href="https://github.com/TikaToka">Github</a>
                </p>
              </td>
              <td style="padding:2.5%;width:37%;max-width:37%">
                <a href="images/jckim.jpg"><img style="width:100%;max-width:100%;object-fit: cover; border-radius: 50%;" alt="profile photo" src="images/jckim.jpg" class="hoverZoomLink"></a>
              </td>
            </tr>
          </tbody></table>
          <div style="text-align: center;">
            <p style="font-weight: bold; color: red;">I am actively looking for a Ph.D. position. If anyone intersted, Please contact me by email or LinkedIn!</p>
          </div>
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
              <tr>
              <td style="padding:16px;width:100%;vertical-align:middle">
                <h2>Research</h2>
                <p>
                  I'm interested in multimodal AI, generative AI, and data-center AI. Most of my research is about understanding video and images with language guidance, thereby developing embodied AI from internet AI. Some papers with <span class="highlight">highlight</span> are papers with main contributions.
                </p>
              </td>
            </tr>
          </tbody></table>
          <table style="width:100%;border:0px;border-spacing:0px 10px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>


    <!-- <tr onmouseout="cat4d_stop()" onmouseover="cat4d_start()" bgcolor="#ffffd0"> -->
          <tr bgcolor="#ffffd0">

            <td style="padding:16px;width:20%;vertical-align:middle">
              <div class="one">
                <!-- <div class="two" id='cat4d_image'><video  width=100% height=100% muted autoplay loop> -->
                <!-- <source src="images/cat4d.mp4" type="video/mp4"> -->
                <!-- Your browser does not support the video tag. -->
                <!-- </video></div> -->
                <img src='images/ordinalbias.png' width="160">
              <!-- </div> -->
              <!-- <script type="text/javascript">
                function cat4d_start() {
                  document.getElementById('cat4d_image').style.opacity = "1";
                }

                function cat4d_stop() {
                  document.getElementById('cat4d_image').style.opacity = "0";
                }
                cat4d_stop()
              </script> -->
            </td>
            <td style="padding:8px;width:80%;vertical-align:middle">
              <!-- <a href="https://cat-4d.github.io/"> -->
            <span class="papertitle">Exploring Ordinal Bias in Action Recognition for Instructional Videos</span>
              <!-- </a> -->
              <br>
              
              <strong>Joochan Kim</strong>,
              <a href="https://minjoong507.github.io">Minjoon Jung</a>,
              <a href="https://cse.snu.ac.kr/en/people/faculty/55">Byoung-Tak Zhang</a>
              <br>
              <!-- <em>CVPR</em>, 2025 &nbsp <font color="red"><strong>(Oral Presentation)</strong></font> -->
              <em>ICLRW</em>, 2025
              <br>
              <a href="data/32.pdf">poster</a>
              /
              <a href="https://arxiv.org/abs/2504.06580">arXiv</a>
              <p></p>
              <p>
                Ordinal bias leads action recognition models to over-rely on dominant action pairs, inflating performance and lacking true video comprehension even when challenged by action masking and sequence shuffling.
              </p>
            </td>
          </tr>

          <!-- <tr bgcolor="#ffffd0"> -->

            <td style="padding:16px;width:20%;vertical-align:middle">
              <div class="one">
                <!-- <div class="two" id='cat4d_image'><video  width=100% height=100% muted autoplay loop> -->
                <!-- <source src="images/cat4d.mp4" type="video/mp4"> -->
                <!-- Your browser does not support the video tag. -->
                <!-- </video></div> -->
                <img src='images/bmdetr.png' width="160">
              </div>
              <!-- <script type="text/javascript">
                function cat4d_start() {
                  document.getElementById('cat4d_image').style.opacity = "1";
                }

                function cat4d_stop() {
                  document.getElementById('cat4d_image').style.opacity = "0";
                }
                cat4d_stop()
              </script> -->
            </td>
            <td style="padding:8px;width:80%;vertical-align:middle">
              <!-- <a href="https://cat-4d.github.io/"> -->
            <span class="papertitle">Background-aware Moment Detection for Video Moment Retrieval</span>
              <!-- </a> -->
              <br>
              <a href="https://minjoong507.github.io">Minjoon Jung</a>,
              <a href='https://github.com/greeksharifa'>Youwon Jang</a>,
              Seongho Choi,
              <strong>Joochan Kim</strong>,
              <a href='https://wityworks.com/'>Jin-Hwa Kim</a>,
              <a href="https://cse.snu.ac.kr/en/people/faculty/55">Byoung-Tak Zhang</a>
              <br>
              <!-- <em>CVPR</em>, 2025 &nbsp <font color="red"><strong>(Oral Presentation)</strong></font> -->
              <em>WACV</em>, 2025
              <br>
              <!-- <a href="https://cat-4d.github.io/">project page</a> -->
              <!-- / -->
              <a href="https://arxiv.org/abs/2306.02728">arXiv</a>
              /
              <a href="https://github.com/minjoong507/BM-DETR">Code</a>
              <p></p>
              <p>
                We propose Background-aware Moment Detection TRansformer (BM-DETR), which carefully adopts a contrastive approach for robust prediction. BM-DETR achieves state-of-the-art performance on various benchmarks while being highly efficient.              </p>
              </p>
            </td>
          </tr>

          <td style="padding:16px;width:20%;vertical-align:middle">
            <div class="one">
              <img src='images/vlncm.png' width="160">
            </div>

          </td>
          <td style="padding:8px;width:80%;vertical-align:middle">
            <!-- <a href="https://cat-4d.github.io/"> -->
          <span class="papertitle">Zero-Shot Vision-and-Language Navigation with Collision Mitigation in Continuous Environment
          </span>
            <!-- </a> -->
            <br>
            Seongjun Jeong,
            <a href='https://gicheonkang.com/'>Gi-Cheon Kang</a>,
            <strong>Joochan Kim</strong>,
            <a href="https://cse.snu.ac.kr/en/people/faculty/55">Byoung-Tak Zhang</a>
            <br>
            <!-- <em>CVPR</em>, 2025 &nbsp <font color="red"><strong>(Oral Presentation)</strong></font> -->
            <em>WACV</em>, 2025
            <br>
            <!-- <a href="https://cat-4d.github.io/">project page</a> -->
            <!-- / -->
            <a href="https://arxiv.org/abs/2410.17267">arXiv</a>
            <p></p>
            <p>
              We propose the zero-shot Vision-and-Language Navigation with Collision Mitigation (VLN-CM), which takes low-level actions as an output while considering possible collisions.</td>
            </p>
          </td>
        </tr>

        <td style="padding:16px;width:20%;vertical-align:middle">
          <div class="one">
            <img src='images/cvln.png' width="160">
          </div>

        </td>
        <td style="padding:8px;width:80%;vertical-align:middle">
        <span class="papertitle">Continual Vision-and-Language Navigation
        </span>
          <br>
          Seongjun Jeong,
          <a href='https://gicheonkang.com/'>Gi-Cheon Kang</a>,
          Seongho Choi,
          <strong>Joochan Kim</strong>,
          <a href="https://cse.snu.ac.kr/en/people/faculty/55">Byoung-Tak Zhang</a>
          <br>
          <em>arXiv Preprint</em>, 2024
          <br>
          <a href="https://arxiv.org/abs/2403.15049">arXiv</a>
          <p></p>
          <p>
            We propose Continual Vision-and-Language Navigation (CVLN) paradigm along with two methods for CVLN: Perplexity Replay (PerpR) and Episodic Self-Replay (ESR).
          </p>
        </td>
      </tr>

      <td style="padding:16px;width:20%;vertical-align:middle">
        <div class="one">
          <img src='images/mpgn.png' width="160">
        </div>

      </td>
      <td style="padding:8px;width:80%;vertical-align:middle">
      <span class="papertitle">Modal-specific Pseudo Query Generation for Video Corpus Moment Retrieval
      </span>
        <br>
        <a href="https://minjoong507.github.io">Minjoon Jung</a>,
        Seongho Choi,
        <strong>Joochan Kim</strong>,
        <a href='https://wityworks.com/'>Jin-Hwa Kim</a>,
        <a href="https://cse.snu.ac.kr/en/people/faculty/55">Byoung-Tak Zhang</a>
        <br>
        <em>EMNLP</em>, 2022
        <br>
        <a href="https://arxiv.org/abs/2210.12617">arXiv</a>
        <p></p>
        <p>
          We propose a self-supervised learning framework: Modal-specific Pseudo Query Generation Network (MPGN). First, MPGN selects candidate temporal moments via subtitle-based moment sampling.Then, it generates pseudo queries exploiting both visual and textual information from the selected temporal moments.
        </p>
      </td>
    </tr>

					<table style="width:100%; margin:0 auto; border:0; border-spacing:0; padding:16px;"><tbody>
            <tr>
              <td>
                <h2>Miscellanea</h2>
              </td>
            </tr>
          </tbody></table>
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            
           
            <!-- <tr>
              <td align="center" style="padding:16px;width:20%;vertical-align:middle">
						     <div class="colored-box" style="background-color: #fcb97d;">
								 <h2>Micropapers</h2>
								 </div>
              </td>
              <td style="padding:8px;width:80%;vertical-align:middle">
                <a href="https://arxiv.org/abs/2112.11687">Squareplus: A Softplus-Like Algebraic Rectifier</a>
                <br>
                <a href="https://arxiv.org/abs/2010.09714">A Convenient Generalization of Schlick's Bias and Gain Functions</a>
                <br>
                <a href="https://arxiv.org/abs/1704.07483">Continuously Differentiable Exponential Linear Units</a>
                <br>
                <a href="https://jonbarron.info/data/cvpr2023_llm_workshop_annotated.pdf">Scholars & Big Models: How Can Academics Adapt?</a>
              </td>
            </tr>


            <tr>
              <td align="center" style="padding:16px;width:20%;vertical-align:middle">
						     <div class="colored-box" style="background-color: #aaba9e;">
								 <h2>Recorded Talks</h2>
								 </div>
              </td>
              <td style="padding:8px;width:80%;vertical-align:center">
                <a href="https://www.youtube.com/watch?v=h9vq_65eDas">View Dependent Podcast, 2024</a>
								<br>
                <a href="https://www.youtube.com/watch?v=4tDhYsFuEqo">Bay Area Robotics Symposium, 2023
</a>
								<br>
                <a href="https://youtu.be/TvWkwDYtBP4?t=7604">EGSR Keynote, 2021</a>
								<br>
								<a href="https://www.youtube.com/watch?v=nRyOzHpcr4Q">TUM AI Lecture Series, 2020</a>
								<br>
								<a href="https://www.youtube.com/watch?v=HfJpQCBTqZs">Vision & Graphics Seminar at MIT, 2020</a>
              </td>
            </tr> -->

            <tr>
              <td align="center" style="padding:16px;width:20%;vertical-align:middle">
						     <div class="colored-box" style="background-color: #c6b89e;">
								 <h2>Academic Service</h2>
								 </div>
              </td>
              <td style="padding:8px;width:80%;vertical-align:center">
                <a href="https://neurips.cc/">Reviewer, NeurIPS 2025</a>
                <!-- <br> -->
              </td>
            </tr>
						
						
           
            <tr>
              <td align="center" style="padding:16px;width:20%;vertical-align:middle">
						     <div class="colored-box" style="background-color: #edd892;">
								 <h2>Teaching</h2>
								 </div>
              </td>
              <td style="padding:8px;width:80%;vertical-align:center">
                Teaching Assistant, M1522.000300 Spring 2023
                <!-- <br>
                <a href="http://inst.eecs.berkeley.edu/~cs188/fa10/announcements.html">Graduate Student Instructor, CS188 Fall 2010</a>
                <br>
                <a href="http://aima.cs.berkeley.edu/">Figures, "Artificial Intelligence: A Modern Approach", 3rd Edition</a> -->
              </td>
            </tr>
            
          </tbody></table>
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
              <td style="padding:0px">
                <br>
                <p style="text-align:right;font-size:small;">
                  Feel free to steal this website's <a href="https://github.com/jonbarron/jonbarron_website">source code</a>. <strong>Do not</strong> scrape the HTML from this page itself, as it includes analytics tags that you do not want on your own website &mdash; use the github code instead. Also, consider using <a href="https://leonidk.com/">Leonid Keselman</a>'s <a href="https://github.com/leonidk/new_website">Jekyll fork</a> of this page.
                </p>
              </td>
            </tr>
          </tbody></table>
        </td>
      </tr>
    </table>
  </body>
</html>
